{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDaIyszzU4xE"
   },
   "source": [
    "Authors: Tim Gorman, Yu Cao, Ling Zhou\n",
    "\n",
    "---\n",
    "\n",
    "Data analyzed in this notebook is from [Kaggle's code competation: Foursquare - Location Matching](https://www.kaggle.com/competitions/foursquare-location-matching/data). The data comprises over one-and-a-half million place entries for hundreds of thousands of commercial Points-of-Interest (POIs) around the globe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MC62YNpkU4xF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# from googletrans import Translator, constants\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "import geopandas as gpd\n",
    "\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "# Note: To install geopandas, open a terminal in jupyter and run the following commands\n",
    "\n",
    "# pip install pipwin \n",
    "# pipwin install gdal \n",
    "# pipwin install fiona \n",
    "# pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Y-BXghYU4xG",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Exploring `train.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic information\n",
    "\n",
    "The official description from [Kaggle](https://www.kaggle.com/competitions/foursquare-location-matching/data):\n",
    "\n",
    "This set comprises eleven attribute fields for over one million place entries, together with:    \n",
    " * `id` - A unique identifier for each entry.  \n",
    " * `point_of_interest` - An identifier for the POI the entry represents. There may be one or many entries describing the same POI. Two entries \"match\" when they describe a common POI.\n",
    " \n",
    " This data set is large (1138812 entries). Later we will decide only work with the file `pairs.csv` for our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyoTKTdIU4xH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "df_train = pd.read_csv(\"../data_raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVXlFNq0U4xH",
    "outputId": "7280fd93-adee-43dc-ff88-050cd163d66e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idtUOlRQU4xH",
    "outputId": "4777ee7f-ac86-462d-9e00-4b10f0130f0d"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waFMskOvU4xI",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Missing values\n",
    "- No missing values in the features: `id`, `latitude`, `longitude`, `point_of_interest`(POI)\n",
    "- All other features have missing values\n",
    "- Features with most missing values (top 3): `url`, `phone`, `zip`.\n",
    "- only one missing value in `name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XYjoxaGU4xI",
    "outputId": "5a502a75-f072-4a0b-c324-45ec663c22d3"
   },
   "outputs": [],
   "source": [
    "df = df_train.copy()\n",
    "# Count the missing values in each feature\n",
    "missing = df.isna().sum()/df.shape[0]*100\n",
    "print(missing.sort_values())\n",
    "\n",
    "# Plot the percentages of missing values for each feature\n",
    "import matplotlib.ticker as mtick\n",
    "tmp = df.isna().sum().sort_values()\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "y = tmp/len(df)*100\n",
    "ax.barh(tmp.index, y)\n",
    "ax.set_title(\"Missing values\", fontsize=18)\n",
    "# ax.axvline(x=60,color='r',linestyle='--')\n",
    "# ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax.tick_params(axis='both', labelsize=15 )\n",
    "ax.set_ylabel('features', fontsize=18)\n",
    "ax.set_xlabel('percentages (%)', fontsize=18)\n",
    "ax.set_xlim(0,100)\n",
    "\n",
    "for index, value in enumerate(y):\n",
    "    if value < 1:\n",
    "        ax.text(value, index, '{:.6f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "    else:\n",
    "        ax.text(value, index, '{:.1f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Features in `train.csv`\n",
    "\n",
    "- We verify that the `id` feature is indeed a unique identifier for each entry.\n",
    "\n",
    "- We explore in this section the features that have fewer missing values:\n",
    "    - latitude and longitude\n",
    "    - name\n",
    "    - country\n",
    "    - point_of_interest\n",
    "\n",
    "- We explore entries that have the same POI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `id` (no missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each value in `id` is indeed unique.\n",
    "print(\"Each value in `id` is unique:\", len(df.id.unique()) == df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `latitude` and `longitude` (no missing values)\n",
    "This (combination of) feature(s) shows that the places are all over the world, mostly in the US and the European regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # fancy plot\n",
    "# geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "# gdf = GeoDataFrame(df_train.copy(), geometry=geometry)   \n",
    "\n",
    "# #this is a simple map that goes with geopandas\n",
    "# world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "# fig, ax = plt.subplots(figsize=(20, 12))\n",
    "# world.plot(figsize=(20, 12), ax=ax)\n",
    "# gdf.plot(ax=ax, marker='o', color='red', markersize=10)\n",
    "\n",
    "# fig.savefig(\"./train_coords.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `name`\n",
    "\n",
    "This column has different languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsg2f55IU4xL",
    "tags": []
   },
   "source": [
    "#### How many languages are present in `name`?\n",
    "\n",
    "We sample 10K names and find that roughly 20% of the sampled data is in English. The second most frequent language is about less than 8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(x):\n",
    "    ''' language detect with exception handling '''\n",
    "    \n",
    "    try:\n",
    "        return detect(x)\n",
    "    except:\n",
    "        # print names that cause exception.\n",
    "        print(x,end=' ')\n",
    "        return np.nan\n",
    "\n",
    "df = df_train.copy()\n",
    "tmp = df['name'].sample(10000).fillna('').str.replace('[{}]'.format(string.punctuation),'',regex=True).str.lower()\n",
    "lang_sample = tmp.apply(lang_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_stats = lang_sample.value_counts()*100/lang_sample.value_counts().sum()\n",
    "lang_stats = lang_stats.head(10).sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "color = [\"gray\"]*len(lang_stats.index)\n",
    "color[-1] = '#1f77b4'\n",
    "lang_stats.plot(kind = 'barh', ax = ax, color = color)\n",
    "\n",
    "ax.set_title(\"languages appearing in the name (top 10)\", fontsize = 18)\n",
    "ax.set_ylabel('language', fontsize = 18)\n",
    "ax.set_xlabel('Percentages (%)', fontsize = 18)\n",
    "for index, value in enumerate(lang_stats.values):\n",
    "    if value < 1:\n",
    "        ax.text(value, index, '{:.6f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "    else:\n",
    "        ax.text(value, index, '{:5.1f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "ax.set_xlim(0,25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `country` \n",
    "\n",
    "Data availability in the `country` feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.copy()\n",
    "country_stats = df['country'].value_counts()*100/df['country'].value_counts().sum()\n",
    "country_stats = country_stats.head(10).sort_values()\n",
    "print(country_stats.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "color = [\"gray\"]*len(country_stats.index)\n",
    "color[-1] = '#1f77b4'\n",
    "country_stats.plot(kind = 'barh', ax = ax, color = color)\n",
    "\n",
    "ax.set_title(\"Available Data by Countries (top 10)\", fontsize = 18)\n",
    "ax.set_ylabel('country', fontsize = 18)\n",
    "ax.set_xlabel('Percentages (%)', fontsize = 18)\n",
    "for index, value in enumerate(country_stats.values):\n",
    "    if value < 1:\n",
    "        ax.text(value, index, '{:.6f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "    else:\n",
    "        ax.text(value, index, '{:5.1f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "ax.set_xlim(0,25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `point_of_interest`\n",
    "\n",
    "Notes on the `point_of_interest` feature:\n",
    "https://www.kaggle.com/competitions/foursquare-location-matching/discussion/318967#1783581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} distinct POIs.'.format(df.point_of_interest.nunique()))\n",
    "print('Abount {:.2f} %POIs are uniue'.format(df.point_of_interest.nunique()*100/df.point_of_interest.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_POI = df['point_of_interest'].value_counts().head(10)\n",
    "most_freq_POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[df['point_of_interest']==most_freq_POI.keys().values[0]].sort_values(by='name')\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[df['point_of_interest']==most_freq_POI.keys().values[4]].sort_values(by='name')\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[df['point_of_interest']==most_freq_POI.keys().values[3]].sort_values(by='name')\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Exploring the US data in `train.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us = df[df['country']=='US'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Missing values\n",
    "- No missing values in the features: `id`, `latitude`, `longitude`, `point_of_interest`(POI)\n",
    "- All other features have missing values\n",
    "- Features with most missing values (top 3): `url`, `phone`, `address`.\n",
    "- only one missing value in `name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_us.copy()\n",
    "# Count the missing values in each feature\n",
    "missing = df.isna().sum()/df.shape[0]*100\n",
    "print(missing.sort_values())\n",
    "\n",
    "# Plot the percentages of missing values for each feature\n",
    "import matplotlib.ticker as mtick\n",
    "tmp = df.isna().sum().sort_values()\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "y = tmp/len(df)*100\n",
    "ax.barh(tmp.index, y)\n",
    "ax.set_title(\"Missing values\", fontsize=18)\n",
    "# ax.axvline(x=60,color='r',linestyle='--')\n",
    "# ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax.tick_params(axis='both', labelsize=15 )\n",
    "ax.set_ylabel('features', fontsize=18)\n",
    "ax.set_xlabel('percentages (%)', fontsize=18)\n",
    "ax.set_xlim(0,100)\n",
    "\n",
    "for index, value in enumerate(y):\n",
    "    if value < 1:\n",
    "        ax.text(value, index, '{:.6f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "    else:\n",
    "        ax.text(value, index, '{:5.1f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `latitude` and `longitude`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NFk48mHU4xN",
    "outputId": "9d61bd58-a318-4453-86fe-3dd65cef4d67"
   },
   "outputs": [],
   "source": [
    "# fancy plot\n",
    "geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "gdf = GeoDataFrame(df.copy(), geometry=geometry)   \n",
    "\n",
    "#this is a simple map that goes with geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "world.plot(figsize=(20, 12), ax=ax)\n",
    "gdf.plot(ax=ax, marker='o', color='red', markersize=10)\n",
    "\n",
    "fig.savefig(\"./train_coords_us.png\")\n",
    "plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(40,10))\n",
    "# ax.scatter(df_us['longitude'],df_us['latitude'])\n",
    "# ax.set_xlabel('longitude',fontsize=40)\n",
    "# ax.set_ylabel('latitude',fontsize=40)\n",
    "# ax.tick_params(axis='both', labelsize=40)\n",
    "# ax.set_title('US data',fontsize=40)\n",
    "# ax.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_us['name'].sort_values()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(x):\n",
    "    ''' language detect with exception handling '''\n",
    "    \n",
    "    try:\n",
    "        return detect(x)\n",
    "    except:\n",
    "        # print names that cause exception.\n",
    "        print(x,end=' ')\n",
    "        return np.nan\n",
    "\n",
    "tmp = df['name'].sample(10000).fillna('').str.replace('[{}]'.format(string.punctuation),'',regex=True).str.lower()\n",
    "lang_sample = tmp.apply(lang_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_stats = lang_sample.value_counts()*100/lang_sample.value_counts().sum()\n",
    "lang_stats = lang_stats.head(10).sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "color = [\"gray\"]*len(lang_stats.index)\n",
    "color[-1] = '#1f77b4'\n",
    "lang_stats.plot(kind = 'barh', ax = ax, color = color)\n",
    "\n",
    "ax.set_title(\"languages appearing in the name (top 10)\", fontsize = 18)\n",
    "ax.set_ylabel('language', fontsize = 18)\n",
    "ax.set_xlabel('Percentages (%)', fontsize = 18)\n",
    "for index, value in enumerate(lang_stats.values):\n",
    "    if value < 1:\n",
    "        ax.text(value, index, '{:.6f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "    else:\n",
    "        ax.text(value, index, '{:5.1f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "ax.set_xlim(0,50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `state`\n",
    "\n",
    "- We can filter non US data by looking at the `state` feature. \n",
    "- It turns out that the state codes are not in the same format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop these.\n",
    "# filter_set = ['国外','UK','CE','ON/NY','Capital Region of Denmark','BCN','Tamaulipas','NU']\n",
    "# df_us[df_us['state'].isin(filter_set)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `categories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.loc[df_us['categories'].notna(),'categories'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `point_of_interest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_us.copy()\n",
    "print('There are {} distinct POIs.'.format(df.point_of_interest.nunique()))\n",
    "print('Abount {:.2f} %POIs are uniue'.format(df.point_of_interest.nunique()*100/df.point_of_interest.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_POI = df['point_of_interest'].value_counts().head(10)\n",
    "most_freq_POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[df['point_of_interest']==most_freq_POI.keys().values[0]].sort_values(by='name')\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[df['point_of_interest']==most_freq_POI.keys().values[1]].sort_values(by='name')\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[df['point_of_interest']==most_freq_POI.keys().values[2]].sort_values(by='name')\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6_CxSi5U4xN",
    "tags": []
   },
   "source": [
    "# Exploring and Manipulating `pairs.csv`\n",
    "\n",
    "Official description of the data:\n",
    "\n",
    "`pairs.csv` - A pregenerated set of pairs of place entries from train.csv designed to improve detection of matches. You may wish to generate additional pairs to improve your model's ability to discriminate POIs.\n",
    "        \n",
    "`match` - Whether (True or False) the pair of entries describes a common POI (in `train.csv`).\n",
    "\n",
    "The file `pairs.csv` is a subset of pairs from `train.csv`. The value of `match` is \"True\" if an only if the pair has the same `point_of_interest` (POI) value in `train.csv`. We will focus on the US pairs. \n",
    "print('There are {} unique id_1s'.format(df_pairs.id_1.nunique()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkQkT7NvU4xN"
   },
   "source": [
    "## Basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3nKi21PU4xN"
   },
   "outputs": [],
   "source": [
    "# # Tim loading\n",
    "# df_pairs = pd.read_csv(r'C:\\Users\\gorma\\OneDrive\\Documents\\Erdos\\foursquare-location-matching\\data_raw\\pairs.csv')\n",
    "# Yu loading\n",
    "df_pairs = pd.read_csv('../data_raw/pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo_BdR09U4xO",
    "outputId": "8aeb2c42-d1d2-4ef6-d6c7-8c4bd2d2d96e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_pairs.shape)\n",
    "df_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} pairs'.format(df_pairs.shape[0]))\n",
    "print('There are {} unique id_1s'.format(df_pairs.id_1.nunique()))\n",
    "print('There are {} unique id_2s'.format(df_pairs.id_2.nunique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJMYK5X9gnKz",
    "outputId": "39ce778c-1e89-4d5c-e052-c3ae8ffda39a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pairs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs_us = df_pairs[(df_pairs.country_1=='US') & (df_pairs.country_2=='US')].copy()\n",
    "df_pairs_us.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Henceforth, we will focus on the pairs data with country_1 and country_2 both being `US`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pairs_us.copy()\n",
    "# Count the missing values in each feature\n",
    "missing = df.isna().sum()/df.shape[0]*100\n",
    "print(missing.sort_values())\n",
    "\n",
    "# Plot the percentages of missing values for each feature\n",
    "import matplotlib.ticker as mtick\n",
    "tmp = df.isna().sum().sort_values()\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "y = tmp/len(df)*100\n",
    "ax.barh(tmp.index, y)\n",
    "ax.set_title(\"Missing values\", fontsize=18)\n",
    "# ax.axvline(x=60,color='r',linestyle='--')\n",
    "# ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax.tick_params(axis='both', labelsize=15 )\n",
    "ax.set_ylabel('features', fontsize=18)\n",
    "ax.set_xlabel('percentages (%)', fontsize=18)\n",
    "ax.set_xlim(0,100)\n",
    "\n",
    "for index, value in enumerate(y):\n",
    "    if value < 1:\n",
    "        ax.text(value, index, '{:.6f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "    else:\n",
    "        ax.text(value, index, '{:5.1f}'.format(value), color = 'red', fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pairs_us.copy()\n",
    "# reindexing the subset.\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing value with empty string.\n",
    "df.fillna('', inplace=True)\n",
    "# df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string_features = ['name_1', 'name_2',\n",
    "                   'address_1', 'address_2', \n",
    "                   'city_1', 'city_2',\n",
    "                   'state_1', 'state_2', \n",
    "                   'zip_1', 'zip_2',\n",
    "                   'country_1', 'country_2', \n",
    "                   'url_1', 'url_2',\n",
    "                   'phone_1', 'phone_2', \n",
    "                   'categories_1', 'categories_2']\n",
    "\n",
    "df[string_features] = df[string_features].fillna('').astype(str).apply(\n",
    "    lambda x: x.str.lower().str.replace('[{}]'.format(string.punctuation),'',regex=True))\n",
    "\n",
    "# The original data type of the position_features is already float64\n",
    "# position_features = ['latitude_1', 'latitude_2', 'longitude_1', 'longitude_2']\n",
    "# df[position_features] = df[position_features].astype('float64')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two letter US state code from \n",
    "# https://www.bls.gov/respondents/mwr/electronic-data-interchange/appendix-d-usps-state-abbreviations-and-fips-codes.htm\n",
    "# saved in state-code.csv\n",
    "# Notes: Don't be confused with FIPS Code: 56 is not the total; some numbers are missing.\n",
    "\n",
    "state = pd.read_csv('../data_raw/state-code.csv', header=None)\n",
    "state.columns=['state','code']\n",
    "state['state'] = state['state'].str.upper()\n",
    "state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify the format of the state code\n",
    "print('before unified:')\n",
    "print(df['state_1'].unique())\n",
    "print(df['state_2'].unique())\n",
    "dict_lookup = dict(zip(state['state'],state['code']))\n",
    "dict_lookup['CALIF']  = 'CA'\n",
    "# dict_lookup['D.C.']   = 'DC'\n",
    "dict_lookup['ONNY']  = 'NY'\n",
    "\n",
    "df['state_1'] = df['state_1'].str.upper()\n",
    "df['state_2'] = df['state_2'].str.upper()\n",
    "\n",
    "for x in dict_lookup.keys():\n",
    "    \n",
    "    df.loc[df[\"state_1\"] == x, \"state_1\"] = dict_lookup[x]\n",
    "    df.loc[df[\"state_2\"] == x, \"state_2\"] = dict_lookup[x]\n",
    "\n",
    "print(78*'*-')\n",
    "print('after unified:')\n",
    "print(df['state_1'].unique())\n",
    "print(df['state_2'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throw away rows with the `state` codes that are not in `state-code.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(df['state_1'].unique()).difference(set(state['code'])))\n",
    "print(set(df['state_2'].unique()).difference(set(state['code'])))\n",
    "S1 = set(df['state_1'].unique())\n",
    "S2 = set(df['state_2'].unique())\n",
    "S  = set(state['code'])\n",
    "drop_list = S1.union(S2).difference(S)\n",
    "print(drop_list)\n",
    "drop_list.remove('')\n",
    "print(drop_list) # drop_list = ['UK', 'CE', 'NU', '国外']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['state_1'].isin(drop_list)==False] \n",
    "df = df[df['state_2'].isin(drop_list)==False] \n",
    "print(df['state_1'].unique())\n",
    "print(df['state_2'].unique())\n",
    "tmp1 = df['state_1'].unique()\n",
    "tmp2 = df['state_2'].unique()\n",
    "print('state code  missed in state_1:')\n",
    "print(set(state['code']).difference(set(tmp1)))\n",
    "print('state code  missed in state_2:')\n",
    "print(set(state['code']).difference(set(tmp2)))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No data pairs in Puerto Rico (PR) or Virgin Islands (VI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDBWCz6BjabY"
   },
   "source": [
    "## Feature analysis\n",
    "\n",
    "---\n",
    "The available pairs of features include:\n",
    "\n",
    "```\n",
    "'name', 'latitude', 'longitude', 'address', 'city', 'state',\n",
    "       'zip', 'country', 'url', 'phone', 'categories'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True vs False in the `match` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.groupby('match')['match'].count()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('False/True: {}'.format(tmp.loc[0]/tmp.loc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subset the data (randomly) further to make it 50/50. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ROFAHSvjp2i"
   },
   "source": [
    "### Comparing the numeric feature (latitude, longitude)\n",
    "\n",
    "\n",
    "---\n",
    "We compute the L^\\infty distance for (latitude, longitude) of each US pair. \n",
    "\n",
    "[The following needs supported examples.]\n",
    "  * Locations with small distance (less than 0.001) may have different POIs: stores are next to each other in a shopping center. In some extreme cases, places with identical coordinates can have different POI. For example, one classroom can be located vertically on top of another.  \n",
    "  * Conversely, locations physically far can represent the same POI, such as mountains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kmvhtodjsOt",
    "outputId": "09ae32d6-30e2-4234-8f08-7d9cc2ef2602"
   },
   "outputs": [],
   "source": [
    "# Compute the L^\\infty difference of (latitude, longitude) of each pair in df_pairs.csv\n",
    "df['location_diff'] = pd.concat([(df['latitude_1']-df['latitude_2']).abs(), \n",
    "                                 (df['longitude_1']-df['longitude_2']).abs()], axis=1).max(axis=1)\n",
    "\n",
    "df.location_diff.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following strip plots show that location difference alone is not useful for determining the POI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "GjRGIvBCjtr1",
    "outputId": "146ddf84-5690-48a8-cd90-de7565ded4ed"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize = (12,6))\n",
    "sns.stripplot(x=\"match\", y=\"location_diff\", data=df.loc[df.location_diff<1], ax=axs[0])\n",
    "axs[0].set_ylabel(r\"location difference ($L^\\infty$)\", fontsize=18)\n",
    "axs[0].set_xlabel(\"match\", fontsize=18)\n",
    "axs[0].set_title('location difference < 1',fontsize=14)\n",
    "axs[0].tick_params(labelsize=14)\n",
    "\n",
    "sns.stripplot(x=\"match\", y=\"location_diff\", data=df.loc[df.location_diff<0.0001], ax=axs[1])\n",
    "axs[1].set_ylabel(r\"location difference ($L^\\infty$)\", fontsize=18)\n",
    "axs[1].set_xlabel(\"match\", fontsize=18)\n",
    "axs[1].set_title('location difference < 0.0001',fontsize=14)\n",
    "axs[1].tick_params(labelsize=14)\n",
    "axs[1].yaxis.tick_right()\n",
    "axs[1].yaxis.set_label_position(\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "lE2QZGghjvZT",
    "outputId": "e4e2b54a-dafd-40a1-9c01-e136ee9c4700"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize = (8,6))\n",
    "# sns.stripplot(x=\"match\", y=\"location_diff\", data=df.loc[df.location_diff==0],ax=ax)\n",
    "# ax.set_ylabel(r\"location difference\", fontsize=18)\n",
    "# ax.set_xlabel(\"match\", fontsize=18)\n",
    "# ax.tick_params(labelsize=14)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myprint(eps):\n",
    "    tmp = len(df.loc[(df.location_diff<=eps) & (df.match==False)])\n",
    "    total = len(df)\n",
    "    if eps == 0:\n",
    "        print('There are {:5d} (US) pairs with location difference ={:.5f} but different POIs, out of all {:2.2f}% (US) pairs.'.format(\n",
    "          tmp, eps, tmp*100/total))\n",
    "    else:\n",
    "        print('There are {:5d} (US) pairs with location difference<={:.5f} but different POIs, out of all {:2.2f}% (US) pairs.'.format(\n",
    "          tmp, eps, tmp*100/total))\n",
    "\n",
    "myprint(1)\n",
    "myprint(0.0001)\n",
    "myprint(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLGuEQP1jwnH",
    "outputId": "24cdda0f-21ec-45bb-f54b-9e184a7cd299"
   },
   "outputs": [],
   "source": [
    "# print('There are',len(df.loc[(df.location_diff==0) & (df.match==False)]),\n",
    "#       'df_pairs with location_diff=0 but different POIs, out of all',len(df_pairs),'df_pairs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeKcOoJMjxm1"
   },
   "outputs": [],
   "source": [
    "# ### name\n",
    "\n",
    "#   * There are some true pairs with slightly different names, e.g. short name v.s. full name.\n",
    "#   * Conversely, there are false pairs with the same name, e.g. chain stores in different cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kk68YMgWj0tW",
    "outputId": "6f6ccd40-74e3-40a9-fb1e-47d6e2a1535a"
   },
   "outputs": [],
   "source": [
    "# df.loc[df.match==True,['name_1','name_2','match']].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing string features using different metrics\n",
    "Most columns in this data are strings. We consider different metrics for string comparison letter by letter, including:\n",
    "*   `SequenceMatcher`\n",
    "*   `Levenshtein distance`\n",
    "\n",
    "or word by word using\n",
    "\n",
    "*  `CountVectorizer` and `cosine_similarity`\n",
    "\n",
    "If one value of a pair of features is missing, we set the distance to be -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff(df,func,*feature,name=None,sim=None):\n",
    "    '''Get difference of given feature using input metric\n",
    "    \n",
    "    df:      data frame of pairs\n",
    "    func:    metric to apply\n",
    "    feature: features to compare with the metric\n",
    "    name:    new feature name\n",
    "    \n",
    "    '''\n",
    "    if name is None:\n",
    "        name = feature[0]+'_diff'\n",
    "    if sim is None:\n",
    "        sim = 'difference'\n",
    "    else:\n",
    "        sim = 'similarity'\n",
    "    \n",
    "    print('Getting {} in {} with the \"{}\" metric.'.format(sim,feature,func.__name__))\n",
    "    \n",
    "    print('Making new column \"{}\"'.format(name))\n",
    "    print()\n",
    "    cols = [str(x)+'_1' for x in feature]+[str(x)+'_2' for x in feature]   \n",
    "    df[name] = df[cols].apply(func, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_metric(pair):\n",
    "    ''' Euclidean distance (l2) for comparing geographic coordinates\n",
    "    \n",
    "    '''\n",
    "    a = np.asarray(pair[0],pair[1])\n",
    "    b = np.asarray(pair[2],pair[3])\n",
    "    return np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "def seq_metric(pair):\n",
    "    ''' metric for comparing strings\n",
    "    \n",
    "    '''\n",
    "    a,b = tuple(pair)\n",
    "    if type(a)!=str or type(b)!=str:\n",
    "        return -1\n",
    "    else:\n",
    "        # from Ling.\n",
    "        a = a.lower().replace(\" \", \"\").replace(\"'\", \"\")\n",
    "        b = b.lower().replace(\" \", \"\").replace(\"'\", \"\")\n",
    "        # b = b.lower().replace('[{}]'.format(string.punctuation),'').replace(\" \", \"\")\n",
    "    return 1-SequenceMatcher(None, a, b).ratio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as lev\n",
    "def lev_metric(pair):\n",
    "    ''' Levenshtein metric for comparing strings\n",
    "    \n",
    "    '''\n",
    "    a,b = tuple(pair)\n",
    "    if type(a)!=str or type(b)!=str:\n",
    "        return -1\n",
    "    elif len(a)==0 or len(b)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        a = a.lower().replace(\" \", \"\").replace(\"'\", \"\")\n",
    "        b = b.lower().replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    return lev(a,b)/max(len(a),len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.values)\n",
    "print()\n",
    "\n",
    "feature = ('latitude','longitude')\n",
    "get_diff(df,l2_metric,*feature,name='geo_diff')\n",
    "\n",
    "features = ['name','address','city','state','zip','url','phone','categories']\n",
    "features = [[x] for x in features]\n",
    "for feature in features:\n",
    "    get_diff(df,seq_metric,*feature,name=feature[0]+'_diff_seq')\n",
    "    get_diff(df,lev_metric,*feature,name=feature[0]+'_diff_lev')\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "\n",
    "def cos_sim(pair):\n",
    "    ''' cosine_sim metric for comparing strings\n",
    "    \n",
    "    '''\n",
    "    a,b = tuple(pair)\n",
    "\n",
    "    try:\n",
    "        if (a == '') | (b == ''):\n",
    "            csim = -1\n",
    "        elif (a == ' ') | (b == ' '):\n",
    "            csim = -1\n",
    "        else:\n",
    "            csim = cosine_similarity(CountVectorizer().fit_transform([a,b]).toarray())[0][1]\n",
    "        return csim\n",
    "    except ValueError:\n",
    "        print(a,b)\n",
    "        csim = -1\n",
    "        return csim\n",
    "    \n",
    "features = ['name','address','city','state','zip','url','phone','categories']\n",
    "features = [[x] for x in features]\n",
    "for feature in features:\n",
    "    get_diff(df,cos_sim,*feature,name=feature[0]+'_csim')\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMVHl-9PkpRK"
   },
   "source": [
    "We now construct the new dataframe with the selected feactures and chosen methods to compute a metric between the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9qZD8FolBSA"
   },
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Q75uUWwAlDei",
    "outputId": "a8ce487e-35f2-4b01-e086-aec837fcf8ed"
   },
   "outputs": [],
   "source": [
    "## histograms for SequenceMatcher\n",
    "\n",
    "columns = ['location_diff','name_diff','address_diff','city_diff','zip_diff','url_diff', 'phone_diff', 'categories_diff']\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    column=columns[i]\n",
    "    plt.hist(df_pairs_seq.loc[(df_pairs_seq.match==True)&(df_pairs_seq.location_diff<1), column], color='b', label=\"True\")\n",
    "    plt.hist(df_pairs_seq.loc[(df_pairs_seq.match==False)&(df_pairs_seq.location_diff<1), column], color='r', label=\"False\")\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title(column,fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PSpEqV7vlEoU",
    "outputId": "2260bc74-c229-4713-850d-e7324ed240ff"
   },
   "outputs": [],
   "source": [
    "# Histogram for Levenshtein distance\n",
    "\n",
    "columns = ['location_diff','name_diff','address_diff','city_diff','zip_diff','url_diff', 'phone_diff', 'categories_diff']\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    column=columns[i]\n",
    "    plt.hist(df_pairs_lev.loc[(df_pairs_lev.match==True)&(df_pairs_lev.location_diff<1), column], color='b', label=\"True\")\n",
    "    plt.hist(df_pairs_lev.loc[(df_pairs_lev.match==False)&(df_pairs_lev.location_diff<1), column], color='r', label=\"False\")\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title(column,fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAyRT6N5U4xU"
   },
   "source": [
    "### Tim Prep for Cosine Similarities: Filling NAs and  Making Combined Full Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyE4moKJU4xU"
   },
   "outputs": [],
   "source": [
    "df_pairs['categories_1']   = df_pairs['categories_1'].fillna('')\n",
    "df_pairs['categories_2']   = df_pairs['categories_2'].fillna('')\n",
    "df_pairs['full_address_1'] = df_pairs['address_1'].fillna('') + ' ' + df_pairs['city_1'].fillna('') + ' ' + df_pairs['state_1'].fillna('') + ' ' + df_pairs['zip_1'].fillna('')  + ' ' + df_pairs['country_1'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaQaD1SLU4xU"
   },
   "outputs": [],
   "source": [
    "df_pairs['full_address_1'] = df_pairs['address_1'].fillna('') + ' ' + df_pairs['city_1'].fillna('') + ' ' + df_pairs['state_1'].fillna('') + ' ' + df_pairs['zip_1'].fillna('')  + ' ' + df_pairs['country_1'].fillna('')\n",
    "df_pairs['full_address_2'] = df_pairs['address_2'].fillna('') + ' ' + df_pairs['city_2'].fillna('') + ' ' + df_pairs['state_2'].fillna('') + ' ' + df_pairs['zip_2'].fillna('')  + ' ' + df_pairs['country_2'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7FTSOWtU4xU"
   },
   "source": [
    "### Reducing to only columns that seems useful. (Mostly based on shear amount of nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfrQhgJEU4xU"
   },
   "outputs": [],
   "source": [
    "column_list = ['id_1','name_1', 'latitude_1', 'longitude_1', 'country_1', 'full_address_1', 'categories_1', 'id_2','name_2', 'latitude_2', 'longitude_2', 'country_2', 'full_address_2', 'categories_2', 'match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6W8TYd17U4xU"
   },
   "outputs": [],
   "source": [
    "pairs_reduced = df_pairs[column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U6kXpEmU4xU",
    "outputId": "5e0b79b6-2358-4dd6-fc0a-20ec4d50acf5"
   },
   "outputs": [],
   "source": [
    "pairs_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DR57jTIKU4xU",
    "outputId": "1e34fabf-7b3a-4c60-a88b-d8b3adac4ec6"
   },
   "outputs": [],
   "source": [
    "pairs_reduced.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfAyUUY8U4xU"
   },
   "source": [
    "### Calculating angular difference of latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hi4CDENPU4xV",
    "outputId": "e4cb56bd-4704-4431-bfcc-1588b3dd7aac"
   },
   "outputs": [],
   "source": [
    "pairs_reduced['theta_diff'] = np.abs(np.arctan(np.sqrt(\n",
    "    (np.cos(np.radians(pairs_reduced['latitude_2']))*np.sin(np.radians(pairs_reduced['longitude_1']-pairs_reduced['longitude_2'])))**2 +\n",
    "        (np.cos(np.radians(pairs_reduced['latitude_1']))*np.sin(np.radians(pairs_reduced['latitude_2']))-\n",
    "            np.sin(np.radians(pairs_reduced['latitude_1']))*np.cos(np.radians(pairs_reduced['latitude_2']))*np.cos(np.radians(pairs_reduced['longitude_1']-pairs_reduced['longitude_2'])))**2\n",
    "                                                )/\n",
    "    (np.sin(np.radians(pairs_reduced['latitude_1']))*np.sin(np.radians(pairs_reduced['latitude_2']))+\n",
    "        np.cos(np.radians(pairs_reduced['latitude_1']))*np.cos(np.radians(pairs_reduced['latitude_2']))*np.cos(np.radians(pairs_reduced['longitude_1']-pairs_reduced['longitude_2']))\n",
    "    )\n",
    "                                               )\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GECDH0nU4xV",
    "outputId": "271cf488-5481-4b26-a074-a50b1dfc2deb"
   },
   "outputs": [],
   "source": [
    "pairs_reduced[pairs_reduced['theta_diff'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qu6X-NR3U4xV",
    "tags": []
   },
   "source": [
    "# Reducing `pairs.csv` to only US data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKy9sHefU4xV",
    "outputId": "039e4d1b-61d8-4bf3-9782-e95c22315361"
   },
   "outputs": [],
   "source": [
    "len(pairs_reduced[(pairs_reduced['country_1'] == 'US') & (pairs_reduced['country_2'] == 'US')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hsoZlxdU4xV"
   },
   "source": [
    "Dropping no longer needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CFueOCeU4xV"
   },
   "outputs": [],
   "source": [
    "pairs_red_us = pairs_reduced[(pairs_reduced['country_1'] == 'US') & (pairs_reduced['country_2'] == 'US')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OozA0cQYU4xV"
   },
   "outputs": [],
   "source": [
    "pairs_red_us = pairs_red_us.drop(['country_1','country_2', 'latitude_1', 'longitude_1', 'latitude_2', 'longitude_2'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYdoE1iWU4xV",
    "outputId": "1aed2aa3-bd2a-41e6-c69e-c163849f9afd"
   },
   "outputs": [],
   "source": [
    "pairs_red_us.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5UWiep-U4xW"
   },
   "source": [
    "It's probably in my best interest to lower case all of the strings when doing the string matching aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhPVnrf5U4xW"
   },
   "outputs": [],
   "source": [
    "pairs_red_us['name_1'] = pairs_red_us['name_1'].astype(str)\n",
    "pairs_red_us['name_2'] = pairs_red_us['name_2'].astype(str)\n",
    "pairs_red_us['full_address_1'] = pairs_red_us['full_address_1'].astype(str)\n",
    "pairs_red_us['full_address_2'] = pairs_red_us['full_address_2'].astype(str)\n",
    "pairs_red_us['categories_1'] = pairs_red_us['categories_1'].astype(str)\n",
    "pairs_red_us['categories_2'] = pairs_red_us['categories_2'].astype(str)\n",
    "\n",
    "pairs_red_us['name_1'] = pairs_red_us['name_1'].str.lower()\n",
    "pairs_red_us['full_address_1'] = pairs_red_us['full_address_1'].str.lower()\n",
    "pairs_red_us['categories_1'] = pairs_red_us['categories_1'].str.lower()\n",
    "pairs_red_us['name_2'] = pairs_red_us['name_2'].str.lower()\n",
    "pairs_red_us['full_address_2'] = pairs_red_us['full_address_2'].str.lower()\n",
    "pairs_red_us['categories_2'] = pairs_red_us['categories_2'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozBHmXePU4xW",
    "outputId": "60e99e19-a33d-454d-da5c-06c86ae80cd7"
   },
   "outputs": [],
   "source": [
    "pairs_red_us.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7V6mmvBXU4xW"
   },
   "source": [
    "I tihnk I can drop ids as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqQ84BbkU4xW"
   },
   "outputs": [],
   "source": [
    "pairs_red_us = pairs_red_us.drop(['id_1', 'id_2'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3cdivbGU4xW",
    "outputId": "868cfdbd-9d18-47c8-814b-acb6f008583d"
   },
   "outputs": [],
   "source": [
    "len(pairs_red_us['name_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lu-hnqpeU4xW",
    "outputId": "069a1245-a6e8-4eba-b07a-2f1ab58e99f8"
   },
   "outputs": [],
   "source": [
    "len(pairs_red_us['name_1'][pairs_red_us['name_1'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWPC4WakU4xW",
    "outputId": "19004d3c-c558-440a-bbde-524bc3062ccb"
   },
   "outputs": [],
   "source": [
    "pairs_red_us['name_1'].tolist()[1:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kIdUhFlU4xW"
   },
   "source": [
    "Now following the article \"Calculating STring Similarity in Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahbyO9GSU4xW",
    "outputId": "03d5de27-79cf-4f34-9e4e-374e5a3d8a92"
   },
   "outputs": [],
   "source": [
    "pairs_red_us['name_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99giZrxYU4xX"
   },
   "outputs": [],
   "source": [
    "pairs_red_us['name_1'] = pairs_red_us['name_1'].str.replace('[{}]'.format(string.punctuation),'')\n",
    "pairs_red_us['name_2'] = pairs_red_us['name_2'].str.replace('[{}]'.format(string.punctuation),'')\n",
    "pairs_red_us['full_address_1'] = pairs_red_us['full_address_1'].str.replace('[{}]'.format(string.punctuation),'')\n",
    "pairs_red_us['full_address_2'] = pairs_red_us['full_address_2'].str.replace('[{}]'.format(string.punctuation),'')\n",
    "pairs_red_us['categories_1'] = pairs_red_us['categories_1'].str.replace('[{}]'.format(string.punctuation),'')\n",
    "pairs_red_us['categories_2'] = pairs_red_us['categories_2'].str.replace('[{}]'.format(string.punctuation),'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWohhtXkU4xX",
    "outputId": "470b486b-9d3f-4778-a414-0128b54420af"
   },
   "outputs": [],
   "source": [
    "pairs_red_us['name_1'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qk_TMHmoU4xX",
    "outputId": "6ed19375-0bd5-4235-ed43-56a72e3d3a3d"
   },
   "outputs": [],
   "source": [
    "pairs_red_us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1MDITeUU4xX"
   },
   "outputs": [],
   "source": [
    "pairs_red_us = pairs_red_us.reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nYUj4d5U4xX",
    "outputId": "e268a84b-5bd1-4987-df32-d68bbaf013f9"
   },
   "outputs": [],
   "source": [
    "pairs_red_us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMk8CgtmU4xX"
   },
   "outputs": [],
   "source": [
    "pairs_red_us.to_csv(r'C:\\Users\\gorma\\OneDrive\\Documents\\Erdos\\foursquare-location-matching\\tim_code\\working_data\\pairs_reduced_us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6-CVK-8U4xX"
   },
   "outputs": [],
   "source": [
    "vectors = CountVectorizer().fit_transform([pairs_red_us['name_1'].iloc[0], pairs_red_us['name_2'].iloc[0]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6SjtQNCU4xX"
   },
   "outputs": [],
   "source": [
    "csim = cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsCd7hunU4xX",
    "outputId": "4a8295c6-0100-40fb-cd4a-4087cebe2ba2"
   },
   "outputs": [],
   "source": [
    "csim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6rv_c5WU4xY",
    "outputId": "3bed2b83-1c10-47e8-aa82-8644bdedb820"
   },
   "outputs": [],
   "source": [
    "csim[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwGOJaLQU4xY",
    "outputId": "17b6c104-3b62-469e-fd66-3e066d87eb49"
   },
   "outputs": [],
   "source": [
    "range(len(pairs_red_us.iloc[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0maKzFrHU4xY"
   },
   "outputs": [],
   "source": [
    "name_cosines = []\n",
    "for i in range(len(pairs_red_us)):\n",
    "    if (pairs_red_us['name_1'].iloc[i] == '') | (pairs_red_us['name_2'].iloc[i] == ''):\n",
    "        csim = -1\n",
    "    else:\n",
    "        csim = cosine_similarity(CountVectorizer().fit_transform([pairs_red_us['name_1'].iloc[i], pairs_red_us['name_2'].iloc[i]]).toarray())[0][1]\n",
    "    name_cosines.append(csim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSpNAIyRU4xY",
    "outputId": "d8c881c5-0f1e-4054-abdc-fce5e308bb40"
   },
   "outputs": [],
   "source": [
    "len(name_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OZKEs7gU4xY"
   },
   "outputs": [],
   "source": [
    "pairs_red_us['name_cosines'] = pd.DataFrame(name_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiU7BblvU4xY",
    "outputId": "ef90b87c-6f54-4c01-ecf6-dfcb618034ad"
   },
   "outputs": [],
   "source": [
    "pairs_red_us.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2acEbjKU4xY",
    "outputId": "e4c7d950-0e04-4e42-f1ad-18b65377dfcb"
   },
   "outputs": [],
   "source": [
    "pairs_red_us[pairs_red_us['name_cosines'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHTMZHeoU4xY",
    "outputId": "a78cee6a-3f1e-4861-a20b-455577aed8cf"
   },
   "outputs": [],
   "source": [
    "pairs_red_us[pairs_red_us['categories_2'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9j233zqAU4xY",
    "outputId": "643f42e4-3e29-49f4-ea6a-c46e58dd7906"
   },
   "outputs": [],
   "source": [
    "pairs_red_us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GCw8oTDU4xY"
   },
   "outputs": [],
   "source": [
    "address_cosines = []\n",
    "for i in range(len(pairs_red_us)):\n",
    "    if (pairs_red_us['full_address_1'].iloc[i] == '') | (pairs_red_us['full_address_2'].iloc[i] == ''):\n",
    "        csim = -1\n",
    "    else:\n",
    "        csim = cosine_similarity(CountVectorizer().fit_transform([pairs_red_us['full_address_1'].iloc[i], pairs_red_us['full_address_2'].iloc[i]]).toarray())[0][1]\n",
    "    address_cosines.append(csim)\n",
    "\n",
    "pairs_red_us['full_address_cosines'] = pd.DataFrame(address_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZhEy5SSU4xZ",
    "outputId": "c5734dae-7251-429e-bd57-0a411572c93c"
   },
   "outputs": [],
   "source": [
    "pairs_red_us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68oIP2HKU4xZ"
   },
   "outputs": [],
   "source": [
    "categories_cosines = []\n",
    "for i in range(len(pairs_red_us)):\n",
    "    if (pairs_red_us['categories_1'].iloc[i] == '') | (pairs_red_us['categories_2'].iloc[i] == ''):\n",
    "        csim = -1\n",
    "    else:\n",
    "        csim = cosine_similarity(CountVectorizer().fit_transform([pairs_red_us['categories_1'].iloc[i], pairs_red_us['categories_2'].iloc[i]]).toarray())[0][1]\n",
    "    categories_cosines.append(csim)\n",
    "    \n",
    "pairs_red_us['categories_cosines'] = pd.DataFrame(categories_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KItRltTU4xZ",
    "outputId": "330668ce-6fe0-4519-beee-0a7966073ea8"
   },
   "outputs": [],
   "source": [
    "pairs_red_us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7NWk1oVU4xZ"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us = pairs_red_us[['theta_diff', 'name_cosines', 'full_address_cosines', 'categories_cosines', 'match']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYR2S0ZZU4xZ",
    "outputId": "b9939efc-8cc1-4dd7-d6c7-a5d647036d23"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ag0xdrrU4xZ"
   },
   "source": [
    "# Exploring the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3Kzs-x9U4xZ",
    "outputId": "4dc17973-862d-4ae3-829c-67ca84e250a9"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us['theta_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37HMtlUYU4xZ",
    "outputId": "39384057-148b-41ab-a681-9bf572614afb"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sb3rJEBFU4xZ"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXEtnZKAU4xZ"
   },
   "outputs": [],
   "source": [
    "pairs_corr = pairs_final_diffed_us.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccoe3_WpU4xZ",
    "outputId": "97f2c402-41fe-45f3-b010-a086038ccfc4"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(pairs_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gT70y1rlU4xa",
    "outputId": "17975063-3825-431f-af20-3eb69d5495b8"
   },
   "outputs": [],
   "source": [
    "pairs_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2q5WukEU4xa",
    "outputId": "1fdb3f39-c53d-408d-c29c-3ff0a9d17768"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us['theta_diff'][pairs_final_diffed_us['match']==True].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQR3PoXoU4xa",
    "outputId": "94bc9931-f898-400d-cff2-ba18f174ab62"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us['theta_diff'][pairs_final_diffed_us['match']==False].describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "yGFdMLwQU4xa"
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "GD76fEIiU4xa"
   },
   "source": [
    "cols_to_scale = ['theta_diff', 'name_cosines', 'full_address_cosines', 'categories_cosines']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "a1rCB6g8U4xa"
   },
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhIRTcUAU4xb",
    "outputId": "e753891f-5e2b-4aae-8483-b06fb8ad61ba"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "oiUDvHIOU4xb"
   },
   "source": [
    "scaler.fit(pairs_final_diffed_us[cols_to_scale])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "WlE7n2-kU4xb"
   },
   "source": [
    "pairs_final_diffed_us[cols_to_scale] = scaler.transform(pairs_final_diffed_us[cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSiw1yFFU4xb",
    "outputId": "fcb34851-494a-4033-d2e4-b065fbb03588"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "skV0rnAWU4xb"
   },
   "source": [
    "pairs_corr = pairs_final_diffed_us.corr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "65R9snczU4xb"
   },
   "source": [
    "pairs_corr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "gbvsQyihU4xb"
   },
   "source": [
    "sns.heatmap(pairs_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvIaSIfQU4xb"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us.to_csv(r'C:\\Users\\gorma\\OneDrive\\Documents\\Erdos\\foursquare-location-matching\\tim_code\\working_data\\pairs_final_diffed_us.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zc-reOTU4xb"
   },
   "source": [
    "## Optional Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPYavjoSU4xb"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us = pd.read_csv(r'C:\\Users\\gorma\\OneDrive\\Documents\\Erdos\\foursquare-location-matching\\tim_code\\working_data\\pairs_final_diffed_us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBgbd6CAU4xb"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us = pairs_final_diffed_us.drop('Unnamed: 0', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6WcZ85UU4xc",
    "outputId": "8a8e4dd3-6113-4a56-de41-931ca4d6a75b"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdThkOFuU4xc",
    "outputId": "e6eb07c5-04e3-4c59-d015-3ba485ba15fb"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-jyIITfU4xc"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "cols_to_scale = ['theta_diff', 'name_cosines', 'full_address_cosines', 'categories_cosines']\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pairs_final_diffed_us[cols_to_scale])\n",
    "pairs_final_diffed_us[cols_to_scale] = scaler.transform(pairs_final_diffed_us[cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jwp7QBL8U4xc",
    "outputId": "42d9bdd5-e773-4326-cba4-e6dab3fd91bc"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JyyNs3qU4xc"
   },
   "outputs": [],
   "source": [
    "pairs_final_diffed_us.to_csv(r'C:\\Users\\gorma\\OneDrive\\Documents\\Erdos\\foursquare-location-matching\\tim_code\\working_data\\pairs_final_diffed_us_scaled.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR79pqXSp7a2"
   },
   "source": [
    "# To improve (to be filled)\n",
    "\n",
    "\n",
    "\n",
    "1.   Generate more pairs: mention how many pairs there are blabla\n",
    "2.   Consider places in other countries blabla\n",
    "3.   Filling in missing values: for example, zip codes can be obtained from the coordinates.\n",
    "4.   Balance the dataset: change the ratio of true/false pairs blabla.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2022.05.25_train_and_pairs_eda.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:geo_env]",
   "language": "python",
   "name": "conda-env-geo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
